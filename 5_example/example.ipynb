{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up: NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "哇...要用NumPy直接來寫個network!!<br>\n",
    "NumPy是用於科學計算的通用框架 (函式庫跟框架的差別要再查查)<br>\n",
    "可通過使用NumPy手刻一個網路的前向與反向傳遞<br>\n",
    "現在來試試看用NumPy實現三階多項式擬合為正弦函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:99, loss:1161.7638298666643\n",
      "step:199, loss:824.6932594122708\n",
      "step:299, loss:586.1739011501846\n",
      "step:399, loss:417.38517478880146\n",
      "step:499, loss:297.94144738046776\n",
      "step:599, loss:213.41680151205318\n",
      "step:699, loss:153.60271616183064\n",
      "step:799, loss:111.27511462948257\n",
      "step:899, loss:81.32186566419584\n",
      "step:999, loss:60.12535913983492\n",
      "step:1099, loss:45.12558520848639\n",
      "step:1199, loss:34.51094678063097\n",
      "step:1299, loss:26.99946256051163\n",
      "step:1399, loss:21.68393503027248\n",
      "step:1499, loss:17.922383298020286\n",
      "step:1599, loss:15.260507685948362\n",
      "step:1699, loss:13.376821628445805\n",
      "step:1799, loss:12.04382413171274\n",
      "step:1899, loss:11.100523367937218\n",
      "step:1999, loss:10.432992924043464\n",
      "Result: y = -0.04254816928011967 + 0.8565911332504037 x + 0.0073402672054135145 x^2 + -0.09330909412190755 x^3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 創造隨機輸入輸出資料\n",
    "x = np.linspace(-math.pi, math.pi, 2000) #在指定的時間間隔內返回均勻分布的數字，所以這個例子是在正負pi間返回2000組數字\n",
    "y = np.sin(x)\n",
    "\n",
    "# 隨機初始化權重\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6 #e表示科學計數法符號 1e-6代表10的-6次方\n",
    "\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(f\"step:{t}, loss:{loss}\")\n",
    "    \n",
    "    # Back propagation to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred*x).sum()\n",
    "    grad_c = (grad_y_pred*x**2).sum()\n",
    "    grad_d = (grad_y_pred*x**3).sum()\n",
    "    \n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "print(f\"Result: y = {a} + {b} x + {c} x^2 + {d} x^3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor概念與NumPy很像，最大的差別是Tensor可以利用GPU加速運算!<br>\n",
    "PyTorch提供許多Tensor上操作的函數。幕後，Tensor也可以跟蹤計算圖和梯度，也可用做科學計算的通用工具<br>\n",
    "用Tensor實現剛剛NumPy所實現的網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:99, loss:5608.47900390625\n",
      "step:199, loss:3756.371826171875\n",
      "step:299, loss:2518.57568359375\n",
      "step:399, loss:1690.819091796875\n",
      "step:499, loss:1136.9091796875\n",
      "step:599, loss:765.9970703125\n",
      "step:699, loss:517.4488525390625\n",
      "step:799, loss:350.77313232421875\n",
      "step:899, loss:238.9150390625\n",
      "step:999, loss:163.785400390625\n",
      "step:1099, loss:113.2826156616211\n",
      "step:1199, loss:79.30513000488281\n",
      "step:1299, loss:56.425228118896484\n",
      "step:1399, loss:41.004188537597656\n",
      "step:1499, loss:30.60055160522461\n",
      "step:1599, loss:23.57510757446289\n",
      "step:1699, loss:18.826112747192383\n",
      "step:1799, loss:15.612659454345703\n",
      "step:1899, loss:13.436006546020508\n",
      "step:1999, loss:11.960025787353516\n",
      "Result: y = 0.03892526775598526 + 0.8156076669692993 x + -0.006715255323797464 x^2 + -0.08747954666614532 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\") #試試看用GPU跑!!\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 隨機初始化權重\n",
    "a = torch.randn((), device=device, dtype= dtype)\n",
    "b = torch.randn((), device=device, dtype= dtype)\n",
    "c = torch.randn((), device=device, dtype= dtype)\n",
    "d = torch.randn((), device=device, dtype= dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    # Forward pass: 計算y的預測值 以 三次多項式擬合正弦值\n",
    "    y_pred = a + b * x + c * x **2 + d * x ** 3\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(f\"step:{t}, loss:{loss}\")\n",
    "    \n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred*x).sum()\n",
    "    grad_c = (grad_y_pred*x ** 2).sum()\n",
    "    grad_d = (grad_y_pred*x ** 3).sum()\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "print(f\"Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "### PyTorch: Tensor & Autograd\n",
    "PyTorch中的Autograd可以自動微分，自動計算NN中的反向傳播<br>\n",
    "使用Autograd時，網路的正向傳播將定義計算圖；圖中的Node為Tensor，Edge為輸入Tensor產生輸出Tensor的函數。<br>\n",
    "通過該Graph進行反向傳播，可輕鬆計算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每個Tensor代表圖中的一個Node。<br>\n",
    "如果x具有x.requires_grad = True的Tensor，則x.grad是另一個Tensor，其保持x相對於某個標量值的梯度。<br>\n",
    "這邊使用Autograd來實現前面所做的例子，來自動實現反向傳播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:99, loss:868.8411865234375\n",
      "step:199, loss:614.483154296875\n",
      "step:299, loss:435.4798583984375\n",
      "step:399, loss:309.4645690917969\n",
      "step:499, loss:220.72369384765625\n",
      "step:599, loss:158.2130126953125\n",
      "step:699, loss:114.16705322265625\n",
      "step:799, loss:83.12327575683594\n",
      "step:899, loss:61.238075256347656\n",
      "step:999, loss:45.80586242675781\n",
      "step:1099, loss:34.9215087890625\n",
      "step:1199, loss:27.243152618408203\n",
      "step:1299, loss:21.825389862060547\n",
      "step:1399, loss:18.00198745727539\n",
      "step:1499, loss:15.30331039428711\n",
      "step:1599, loss:13.398152351379395\n",
      "step:1699, loss:12.052997589111328\n",
      "step:1799, loss:11.103099822998047\n",
      "step:1899, loss:10.432229042053223\n",
      "step:1999, loss:9.958356857299805\n",
      "Result: y = -0.03537272661924362 + 0.861544668674469 x + 0.006102385465055704 x^2 + -0.09401369094848633 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# 創建虛擬的input和對應的output\n",
    "# 預設的requires_grad = False \n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad = True indicates that we want to compute gradients with respect to these Tensors during the backward pass!!\n",
    "\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicated y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "    \n",
    "    # loss is a Tensor of shape (1, )\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum() #MSE的loss計算\n",
    "    if t % 100 == 99:\n",
    "        print(f\"step:{t}, loss:{loss.item()}\")\n",
    "    \n",
    "    # Use autograd to compute the backward pass.\n",
    "    # This call will compute the gradient of loss with respect to \"all Tensors with requires_grad = True\"!!\n",
    "    # After this call a.grad, b.grad, c.grad and d.grad will be Tensors holding the gradient of the loss with respect to a, b, c, d respectively\n",
    "    loss.backward()\n",
    "    \n",
    "    # 使用梯度下降法手動更新權重！\n",
    "    # Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this in autograd.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        \n",
    "        # 在更新權重後記得將累積梯度全數歸零～～\n",
    "        a.grad=None\n",
    "        b.grad=None\n",
    "        c.grad=None\n",
    "        d.grad=None\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: 定義新的Autograd函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在背後，每個原始的Autograd運算符實際上都是在Tensor上運行的兩個函數。<br>\n",
    "正向函數從輸入Tensor計算輸出Tensor。<br>\n",
    "反向函數接收相對於某個標量值的輸出Tensor的梯度，並計算相對於相同標量值的輸入Tensor的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中，可以通過定義**torch.autograd.Function的子類並實現forward與backward函數**來輕鬆定義自己的Autograd運算符。  \n",
    "然後，我們可以通過構造實例並像調用函數一樣使用新的Autograd運算符，並傳遞包含輸入數據的Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 99, loss: 209.95834350585938\n",
      "step: 199, loss: 144.66018676757812\n",
      "step: 299, loss: 100.70249938964844\n",
      "step: 399, loss: 71.03519439697266\n",
      "step: 499, loss: 50.97850799560547\n",
      "step: 599, loss: 37.403133392333984\n",
      "step: 699, loss: 28.206867218017578\n",
      "step: 799, loss: 21.973188400268555\n",
      "step: 899, loss: 17.7457275390625\n",
      "step: 999, loss: 14.877889633178711\n",
      "step: 1099, loss: 12.931766510009766\n",
      "step: 1199, loss: 11.610918045043945\n",
      "step: 1299, loss: 10.714258193969727\n",
      "step: 1399, loss: 10.10548210144043\n",
      "step: 1499, loss: 9.692106246948242\n",
      "step: 1599, loss: 9.411375045776367\n",
      "step: 1699, loss: 9.220745086669922\n",
      "step: 1799, loss: 9.091285705566406\n",
      "step: 1899, loss: 9.003361701965332\n",
      "step: 1999, loss: 8.943639755249023\n",
      "Result: y = -5.423830273798558e-09 + -2.208526849746704 * P3(1.3320399228078372e-09 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "# 這個案例中，將模型定義為 y = a + b p[3] (c + dx)\n",
    "# p[3](x) = 1/2 (5x ^ 3 - 3x) 是三次的勒讓德多項式。\n",
    "# 這個案例來編寫自定義的Autograd function來計算p[3]的前進與後退\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing torch.autograd.Function\n",
    "    and implementing the forward and backward passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a Tensor containing the output.\n",
    "        ctx is a context object that can be used to stash information for backward computation.\n",
    "        You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output,\n",
    "        and we need to compute the gradient of the loss with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "    \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device = device, dtype = dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "a = torch.full((), 0.0, device = device, dtype = dtype, requires_grad = True)\n",
    "b = torch.full((), -1.0, device = device, dtype = dtype, requires_grad = True)\n",
    "c = torch.full((), 0.0, device = device, dtype = dtype, requires_grad = True)\n",
    "d = torch.full((), 0.3, device = device, dtype = dtype, requires_grad = True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    P3 = LegendrePolynomial3.apply # 別名叫做P3\n",
    "    \n",
    "    # Forward pass: compute predicted y using operations; we compute P3 using our custom autograd operation.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(f\"step: {t}, loss: {loss.item()}\")\n",
    "    \n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新權重\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "        \n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "構建大型神經網路，原始的Autograd可能不夠用  \n",
    "因此在構建nn時，我們會想將計算安排在Layer中，某些Layer具有可學習的參數  \n",
    "這些參數會在學習期間進行優化。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow中，像是Keras, TensorFlow-Slim, TFLearn之類的包在原始計算圖上提供了更高層次的抽象，可用於構建NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中，nn包達到一樣的目的。  \n",
    "nn定義了一組Module，大致等效於神經網路層。  \n",
    "Module接收輸入Tensor並計算輸出Tensor，也可以保持內部狀態，例如包含可學習參數的Tensor  \n",
    "nn包還定義了一組有用的loss function，可在訓練nn時做使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 99, loss: 654.8340454101562\n",
      "step: 199, loss: 437.47344970703125\n",
      "step: 299, loss: 293.3116455078125\n",
      "step: 399, loss: 197.67909240722656\n",
      "step: 499, loss: 134.2257080078125\n",
      "step: 599, loss: 92.11399841308594\n",
      "step: 699, loss: 64.15941619873047\n",
      "step: 799, loss: 45.59776306152344\n",
      "step: 899, loss: 33.26963806152344\n",
      "step: 999, loss: 25.079345703125\n",
      "step: 1099, loss: 19.63631820678711\n",
      "step: 1199, loss: 16.01784896850586\n",
      "step: 1299, loss: 13.611520767211914\n",
      "step: 1399, loss: 12.010741233825684\n",
      "step: 1499, loss: 10.945393562316895\n",
      "step: 1599, loss: 10.236116409301758\n",
      "step: 1699, loss: 9.763710021972656\n",
      "step: 1799, loss: 9.4489164352417\n",
      "step: 1899, loss: 9.23903751373291\n",
      "step: 1999, loss: 9.099055290222168\n",
      "Result: y = -0.007070383056998253 + 0.8417672514915466 x + 0.0012197582982480526 x^2 + -0.09120053052902222 x^3\n"
     ]
    }
   ],
   "source": [
    "# 使用nn構建多項式模型網路\n",
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 這個案例中， 輸出y是一個(x, x^2, x^3)的線性函數 \n",
    "# 所以可以看做線性神經網路，先準備tensor(x, x^2, x^3)\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# x.unsqueeze(-1) has shape (2000, 1), and p has shape(3, ), for this case, broadcasting semantics will apply to obtain a tensor of shape (2000, 3)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers.\n",
    "# nn.Sequential is a Module which contains other Modules, and applies them in sequence to produce its output.\n",
    "# The Linear Module computes output from input using a linear function, and holds interal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor, to match the shape of 'y'.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions!!\n",
    "# in this case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 前向傳播，藉由傳遞x to the model來計算預測值y\n",
    "    # Module objects override the __call__ operator so you can call them like functions.\n",
    "    # When doing so you pass a Tensor of input data to the Module and it produces a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    # 計算loss\n",
    "    # 我們傳遞Tensors 包含預測和實際的y值，以及loss function會返回一個包含loss的Tensor\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(f\"step: {t}, loss: {loss.item()}\")\n",
    "    \n",
    "    # 在跑backward之前要先將梯度值歸零\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable parameters of the model.\n",
    "    # Internally, the parameters of each Module are stored in Tensors with requires_grad = True, so this call will compute gradients for all learnable parametters in the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # 用梯度下降法更新權重\n",
    "    # 每個參數都是一個Tensor，我們可以用先前的做法存取梯度\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "            \n",
    "# You can access the first layer of 'model' like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as 'weight' and 'bias'.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: optim\n",
    "先前都是使用torch.no_grad() 手動更改可學習參數的Tensor來更新模型的權重  \n",
    "對於隨機梯度下降來說，這很easy.  \n",
    "但實戰中，正常更新參數的優化器都會使用更複雜的(e.g. AdaGrad, RMSProp, Adam等) 來訓練NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中的optim包抽象了優化器的方法，並提供常用優化算法的實現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個案例使用optim包提供的RMSprop來優化model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 99, loss: 23213.193359375\n",
      "step: 199, loss: 10697.0380859375\n",
      "step: 299, loss: 5156.4345703125\n",
      "step: 399, loss: 3104.404296875\n",
      "step: 499, loss: 2487.3955078125\n",
      "step: 599, loss: 2176.162841796875\n",
      "step: 699, loss: 1886.525390625\n",
      "step: 799, loss: 1611.3702392578125\n",
      "step: 899, loss: 1362.6876220703125\n",
      "step: 999, loss: 1142.302490234375\n",
      "step: 1099, loss: 947.1959838867188\n",
      "step: 1199, loss: 773.845947265625\n",
      "step: 1299, loss: 620.3523559570312\n",
      "step: 1399, loss: 485.9271240234375\n",
      "step: 1499, loss: 370.0064392089844\n",
      "step: 1599, loss: 272.1261291503906\n",
      "step: 1699, loss: 191.9249267578125\n",
      "step: 1799, loss: 127.76820373535156\n",
      "step: 1899, loss: 79.59146118164062\n",
      "step: 1999, loss: 45.88486862182617\n",
      "Result: y = -0.00030105706537142396 + 0.6701569557189941 x + -0.0003019354771822691 x^2 + -0.0662439838051796 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 重頭戲～ use optim package to define an Optimizer that will update the weights of the model for us.\n",
    "# Here we will use RMSprop\n",
    "# the optim package contains many other optimization algorithms.\n",
    "# The first argument to the RMSprop constructor tells the optimizer \"which Tensors it should update\".\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y by passing x to the model\n",
    "    y_pred = model(xx)\n",
    "    \n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(f\"step: {t}, loss: {loss.item()}\")\n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the gradients for the variables it will update (which are the learnable weights of the model).\n",
    "    # This is because by default, gradients are accumulated in buffers( i.e, not overwritten) whenever .backward() is called.\n",
    "    # Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "linear_layer = model[0]\n",
    "\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: 自定義 nn Module\n",
    "有時候有些模型Sequential絕對做不出來，對於這種情況可使用nn.Module並定義一個forward來定義自己的Module<br>\n",
    "該Module使用其他Module或在Tensors上的其他自動轉換操作來接收輸入Tensors並生成輸出Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#三階多項式實現自定義Module subclasses\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameter.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82d4c6f819cf47785f735f902f00da8643513d08dab4f4c7470bccf934b8d2d6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
